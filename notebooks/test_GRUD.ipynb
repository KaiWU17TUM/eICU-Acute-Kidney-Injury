{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471cd909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c4d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1, np.nan, 3, np.nan],\n",
    "                  [np.nan, 2, 3, np.nan]])\n",
    "a_mean = torch.Tensor([1, 2, 3, 4]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a332a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4),\n",
       " array([[ 1., nan,  3., nan],\n",
       "        [nan,  2.,  3., nan]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, np.nan, 3, np.nan],\n",
    "              [np.nan, 2, 3, np.nan]])\n",
    "a.shape, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e3b08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mean = a_mean.numpy()\n",
    "a_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38cbf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4.],\n",
       "       [1., 2., 3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mean = a_mean.repeat(a.shape[0], axis=0)\n",
    "a_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e25a6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4.],\n",
       "       [1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[np.isnan(a)] = a_mean[np.isnan(a)]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a83ee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8fa7480efc12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "a[a.isnan()] = a_mean[a.isnan()]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class GRUD(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, x_mean=0,\\\n",
    "                 bias=True, batch_first=False, bidirectional=False, dropout_type='mloss', dropout=0):\n",
    "        super(GRUD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.zeros = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        self.x_mean = torch.autograd.Variable(torch.tensor(x_mean))\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout_type = dropout_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n",
    "                isinstance(dropout, bool):\n",
    "            raise ValueError(\"dropout should be a number in range [0, 1] \"\n",
    "                             \"representing the probability of an element being \"\n",
    "                             \"zeroed\")\n",
    "        if dropout > 0 and num_layers == 1:\n",
    "            warnings.warn(\"dropout option adds dropout after all but last \"\n",
    "                          \"recurrent layer, so non-zero dropout expects \"\n",
    "                          \"num_layers greater than 1, but got dropout={} and \"\n",
    "                          \"num_layers={}\".format(dropout, num_layers))\n",
    "        \n",
    "        ################################\n",
    "        gate_size = 1 # not used\n",
    "        ################################\n",
    "        \n",
    "        self._all_weights = []\n",
    "\n",
    "        '''\n",
    "        w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))\n",
    "        w_hh = Parameter(torch.Tensor(gate_size, hidden_size))\n",
    "        b_ih = Parameter(torch.Tensor(gate_size))\n",
    "        b_hh = Parameter(torch.Tensor(gate_size))\n",
    "        layer_params = (w_ih, w_hh, b_ih, b_hh)\n",
    "        '''\n",
    "        # decay rates gamma\n",
    "        w_dg_x = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # z\n",
    "        w_xz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hz = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # r\n",
    "        w_xr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hr = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hh = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # y (output)\n",
    "        w_hy = torch.nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_z = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_r = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_y = torch.nn.Parameter(torch.Tensor(output_size))\n",
    "\n",
    "        layer_params = (w_dg_x, w_dg_h,\\\n",
    "                        w_xz, w_hz, w_mz,\\\n",
    "                        w_xr, w_hr, w_mr,\\\n",
    "                        w_xh, w_hh, w_mh,\\\n",
    "                        w_hy,\\\n",
    "                        b_dg_x, b_dg_h, b_z, b_r, b_h, b_y)\n",
    "\n",
    "        param_names = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                       'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                       'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                       'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                       'weight_hy']\n",
    "        if bias:\n",
    "            param_names += ['bias_dg_x', 'bias_dg_h',\\\n",
    "                            'bias_z',\\\n",
    "                            'bias_r',\\\n",
    "                            'bias_h',\\\n",
    "                            'bias_y']\n",
    "        \n",
    "        for name, param in zip(param_names, layer_params):\n",
    "            setattr(self, name, param)\n",
    "        self._all_weights.append(param_names)\n",
    "\n",
    "        self.flatten_parameters()\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets parameter data pointer so that they can use faster code paths.\n",
    "        Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
    "        Otherwise, it's a no-op.\n",
    "        \"\"\"\n",
    "        any_param = next(self.parameters()).data\n",
    "        if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):\n",
    "            return\n",
    "\n",
    "        # If any parameters alias, we fall back to the slower, copying code path. This is\n",
    "        # a sufficient check, because overlapping parameter buffers that don't completely\n",
    "        # alias would break the assumptions of the uniqueness check in\n",
    "        # Module.named_parameters().\n",
    "        all_weights = self._flat_weights\n",
    "        unique_data_ptrs = set(p.data_ptr() for p in all_weights)\n",
    "        if len(unique_data_ptrs) != len(all_weights):\n",
    "            return\n",
    "\n",
    "        with torch.cuda.device_of(any_param):\n",
    "            import torch.backends.cudnn.rnn as rnn\n",
    "\n",
    "            # NB: This is a temporary hack while we still don't have Tensor\n",
    "            # bindings for ATen functions\n",
    "            with torch.no_grad():\n",
    "                # NB: this is an INPLACE function on all_weights, that's why the\n",
    "                # no_grad() is necessary.\n",
    "                torch._cudnn_rnn_flatten_weight(\n",
    "                    all_weights, (4 if self.bias else 2),\n",
    "                    self.input_size, rnn.get_cudnn_mode(self.mode), self.hidden_size, self.num_layers,\n",
    "                    self.batch_first, bool(self.bidirectional))\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(GRUD, self)._apply(fn)\n",
    "        self.flatten_parameters()\n",
    "        return ret\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        is_input_packed = batch_sizes is not None\n",
    "        expected_input_dim = 2 if is_input_packed else 3\n",
    "        if input.dim() != expected_input_dim:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_dim, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "        if is_input_packed:\n",
    "            mini_batch = int(batch_sizes[0])\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        expected_hidden_size = (self.num_layers * num_directions,\n",
    "                                mini_batch, self.hidden_size)\n",
    "        \n",
    "        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "            if tuple(hx.size()) != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                              'Expected hidden[0] size {}, got {}')\n",
    "            check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                              'Expected hidden[1] size {}, got {}')\n",
    "        else:\n",
    "            check_hidden_size(hidden, expected_hidden_size)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "            s += ', dropout={dropout}'\n",
    "        if self.bidirectional is not False:\n",
    "            s += ', bidirectional={bidirectional}'\n",
    "        return s.format(**self.__dict__)\n",
    "    \n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        super(GRUD, self).__setstate__(d)\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        self._all_weights = []\n",
    "\n",
    "        weights = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                   'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                   'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                   'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                   'weight_hy',\\\n",
    "                   'bias_dg_x', 'bias_dg_h',\\\n",
    "                   'bias_z', 'bias_r', 'bias_h','bias_y']\n",
    "\n",
    "        if self.bias:\n",
    "            self._all_weights += [weights]\n",
    "        else:\n",
    "            self._all_weights += [weights[:2]]\n",
    "\n",
    "    @property\n",
    "    def _flat_weights(self):\n",
    "        return list(self._parameters.values())\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input.size = (3, 33,49) : num_input or num_hidden, num_layer or step\n",
    "        X = torch.squeeze(input[0]) # .size = (33,49)\n",
    "        Mask = torch.squeeze(input[1]) # .size = (33,49)\n",
    "        Delta = torch.squeeze(input[2]) # .size = (33,49)\n",
    "        Hidden_State = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        \n",
    "        step_size = X.size(1) # 49\n",
    "        #print('step size : ', step_size)\n",
    "        \n",
    "        output = None\n",
    "        h = Hidden_State\n",
    "\n",
    "        # decay rates gamma\n",
    "        w_dg_x = getattr(self, 'weight_dg_x')\n",
    "        w_dg_h = getattr(self, 'weight_dg_h')\n",
    "\n",
    "        #z\n",
    "        w_xz = getattr(self, 'weight_xz')\n",
    "        w_hz = getattr(self, 'weight_hz')\n",
    "        w_mz = getattr(self, 'weight_mz')\n",
    "\n",
    "        # r\n",
    "        w_xr = getattr(self, 'weight_xr')\n",
    "        w_hr = getattr(self, 'weight_hr')\n",
    "        w_mr = getattr(self, 'weight_mr')\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = getattr(self, 'weight_xh')\n",
    "        w_hh = getattr(self, 'weight_hh')\n",
    "        w_mh = getattr(self, 'weight_mh')\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = getattr(self, 'bias_dg_x')\n",
    "        b_dg_h = getattr(self, 'bias_dg_h')\n",
    "        b_z = getattr(self, 'bias_z')\n",
    "        b_r = getattr(self, 'bias_r')\n",
    "        b_h = getattr(self, 'bias_h')\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            \n",
    "            x = torch.squeeze(X[:,layer:layer+1])\n",
    "            m = torch.squeeze(Mask[:,layer:layer+1])\n",
    "            d = torch.squeeze(Delta[:,layer:layer+1])\n",
    "\n",
    "\n",
    "            #(4)\n",
    "            gamma_x = torch.exp(-torch.max(self.zeros, (w_dg_x * d + b_dg_x)))\n",
    "            gamma_h = torch.exp(-torch.max(self.zeros, (w_dg_h * d + b_dg_h)))\n",
    "\n",
    "            #(5)\n",
    "            x = m * x + (1 - m) * (gamma_x * x + (1 - gamma_x) * self.x_mean)\n",
    "\n",
    "            #(6)\n",
    "            if self.dropout == 0:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'Moon':\n",
    "                '''\n",
    "                RNNDROP: a novel dropout for rnn in asr(2015)\n",
    "                '''\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "            elif self.dropout_type == 'Gal':\n",
    "                '''\n",
    "                A Theoretically grounded application of dropout in recurrent neural networks(2015)\n",
    "                '''\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'mloss':\n",
    "                '''\n",
    "                recurrent dropout without memory loss arXiv 1603.05118\n",
    "                g = h_tilde, p = the probability to not drop a neuron\n",
    "                '''\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h_tilde = dropout(h_tilde)\n",
    "\n",
    "                h = (1 - z)* h + z*h_tilde\n",
    "\n",
    "            else:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        w_hy = getattr(self, 'weight_hy')\n",
    "        b_y = getattr(self, 'bias_y')\n",
    "\n",
    "        output = torch.matmul(w_hy, h) + b_y\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-py36",
   "language": "python",
   "name": "torch-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
